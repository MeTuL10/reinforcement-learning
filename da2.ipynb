{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT PROMPT:\n",
    "can you create a gym environment for 6X6 grid with start, goal and walls with a gui for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import tkinter as tk\n",
    "import time\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shape = (4,4)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (3,3)\n",
    "        self.walls = [(2, 2), (3, 2), (4, 2), (1, 4), (2, 4), (3, 4)]\n",
    "        self.current_state = self.start_state\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(self.shape[0] * self.shape[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self.current_state\n",
    "        if action == 0:  # up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # down\n",
    "            row = min(row + 1, self.shape[0] - 1)\n",
    "        elif action == 2:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 3:  # right\n",
    "            col = min(col + 1, self.shape[1] - 1)\n",
    "        self.current_state = (row, col)\n",
    "        reward = self._get_reward()\n",
    "        done = self._is_done()\n",
    "        info = {}\n",
    "        return self.current_state, reward, done, info\n",
    "\n",
    "    def _get_reward(self):\n",
    "        if self.current_state == self.goal_state:\n",
    "            return 1\n",
    "        #elif self.current_state in self.walls:\n",
    "        #    return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def _is_done(self):\n",
    "        return self.current_state == self.goal_state \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            if not hasattr(self, 'root'):\n",
    "                self.root = tk.Tk()\n",
    "                self.canvas = tk.Canvas(self.root, width=300, height=300)\n",
    "                self.canvas.pack()\n",
    "                self.root.update()\n",
    "            self.canvas.delete('all')\n",
    "            cell_width = 300 // self.shape[1]\n",
    "            cell_height = 300 // self.shape[0]\n",
    "            for i in range(self.shape[0]):\n",
    "                for j in range(self.shape[1]):\n",
    "                    x1 = j * cell_width\n",
    "                    y1 = i * cell_height\n",
    "                    x2 = (j + 1) * cell_width\n",
    "                    y2 = (i + 1) * cell_height\n",
    "                    fill_color = 'white'\n",
    "                    if (i, j) == self.current_state:\n",
    "                        fill_color = 'green'\n",
    "                    elif (i, j) == self.goal_state:\n",
    "                        fill_color = 'blue'\n",
    "                    #elif (i, j) in self.walls:\n",
    "                    #    fill_color = 'gray'\n",
    "                    self.canvas.create_rectangle(x1, y1, x2, y2, fill=fill_color)\n",
    "            #time.sleep(0.1)\n",
    "            self.root.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT PROMPT:what rl models can be applied in this environment\\\n",
    "\n",
    "response:The grid world environment described can be used to train and test a variety of reinforcement learning algorithms, including but not limited to:\n",
    "\n",
    "Q-learning: A model-free algorithm that learns the optimal action-value function by iteratively updating the Q-values based on the rewards obtained and the estimated future rewards.\n",
    "\n",
    "SARSA (State-Action-Reward-State-Action): Another model-free algorithm that learns the optimal policy by estimating the expected rewards for taking an action in a given state and following a policy thereafter.\n",
    "Policy gradient methods: These are model-free algorithms that optimize the policy directly to maximize the expected cumulative reward.\n",
    "\n",
    "Actor-critic methods: These are model-based algorithms that use a combination of value-based and policy-based methods to learn the optimal policy.\n",
    "\n",
    "Deep reinforcement learning algorithms: These are extensions of the above algorithms that incorporate deep neural networks to learn the Q-values, policies, or both.\n",
    "\n",
    "The choice of the algorithm depends on the specific requirements of the problem, such as the complexity of the environment, the dimensionality of the state and action spaces, the availability of a simulator or model, and the desired performance metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT PROMPT: can you write a sarasa program for the 6X6 grid envirnoment you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = GridWorldEnv()  # create the environment\n",
    "\n",
    "# SARSA parameters\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "# Q table initialization\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# SARSA algorithm\n",
    "for episode in range(70):\n",
    "    state = env.reset()\n",
    "    action = env.action_space.sample()  # choose random action\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        s=state[0]*4+state[1]\n",
    "        s1=next_state[0]*4+next_state[1]\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_action=np.argmax(q_table[s1,:])%4\n",
    "        q_table[s,action] = q_table[s,action] + alpha * (reward + (gamma * q_table[s1,next_action]) - q_table[s,action])\n",
    "        env.render()\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "# Evaluate the learned policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.70929795 -2.5765467  -2.78906422 -2.55943119]\n",
      " [-1.99599665 -2.00345606 -2.02746569 -2.01152485]\n",
      " [-1.53772699 -1.46437193 -1.53446727 -1.50589527]\n",
      " [-1.16492607 -1.01716845 -1.1147356  -1.04361337]\n",
      " [-2.00763724 -2.03227034 -2.14450133 -2.03322799]\n",
      " [-1.62850636 -1.60289093 -1.67199946 -1.60424485]\n",
      " [-1.09042087 -1.04199204 -1.03947216 -1.04158663]\n",
      " [-0.52442392 -0.18844946 -0.45069555 -0.69074998]\n",
      " [-1.59915403 -1.49674577 -1.60390818 -1.53647656]\n",
      " [-1.26095098 -1.04563157 -1.1877521  -1.02248816]\n",
      " [-0.46624681 -0.38820351 -0.44543984 -0.19947997]\n",
      " [-0.1         0.98922474 -0.12675768  0.        ]\n",
      " [-1.1182417  -1.01658033 -1.1711956  -1.04246665]\n",
      " [-0.51606753 -0.64145337 -0.52991512 -0.27928901]\n",
      " [-0.21559265 -0.19767718 -0.199       0.94185026]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
