{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m      7\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m----> 8\u001b[0m     env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m     10\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:299\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    298\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock\u001b[39m.\u001b[39mtick(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mrender_fps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 299\u001b[0m     pygame\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mflip()\n\u001b[0;32m    301\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    303\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m   action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m   \u001b[39m# Choose the action with the highest Q-value (exploitation)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m   action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table\u001b[39m.\u001b[39mflatten()[(state \u001b[39m*\u001b[39m \u001b[39mint\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn),) : ((state \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39m \u001b[39mint\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn),)])\n\u001b[0;32m     28\u001b[0m \u001b[39m# Take a step in the environment\u001b[39;00m\n\u001b[0;32m     29\u001b[0m next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Define the hyperparameters\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.1 # exploration rate\n",
    "\n",
    "# Define the training loop\n",
    "for episode in range(1000):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  \n",
    "  while not done:\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "      # Choose a random action (exploration)\n",
    "      action = env.action_space.sample()\n",
    "    else:\n",
    "      # Choose the action with the highest Q-value (exploitation)\n",
    "      action = np.argmax(q_table.flatten()[(state * int(env.action_space.n),) : ((state + 1) * int(env.action_space.n),)])\n",
    "\n",
    "    # Take a step in the environment\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update the Q-value for the current state-action pair\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]))\n",
    "    \n",
    "    # Set the next state as the current state\n",
    "    state = next_state\n",
    "\n",
    "# Print the final Q-table\n",
    "print(q_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make in module gym.envs.registration:\n",
      "\n",
      "make(id: Union[str, gym.envs.registration.EnvSpec], max_episode_steps: Optional[int] = None, autoreset: bool = False, apply_api_compatibility: Optional[bool] = None, disable_env_checker: Optional[bool] = None, **kwargs) -> gym.core.Env\n",
      "    Create an environment according to the given ID.\n",
      "    \n",
      "    To find all available environments use `gym.envs.registry.keys()` for all valid ids.\n",
      "    \n",
      "    Args:\n",
      "        id: Name of the environment. Optionally, a module to import can be included, eg. 'module:Env-v0'\n",
      "        max_episode_steps: Maximum length of an episode (TimeLimit wrapper).\n",
      "        autoreset: Whether to automatically reset the environment after each episode (AutoResetWrapper).\n",
      "        apply_api_compatibility: Whether to wrap the environment with the `StepAPICompatibility` wrapper that\n",
      "            converts the environment step from a done bool to return termination and truncation bools.\n",
      "            By default, the argument is None to which the environment specification `apply_api_compatibility` is used\n",
      "            which defaults to False. Otherwise, the value of `apply_api_compatibility` is used.\n",
      "            If `True`, the wrapper is applied otherwise, the wrapper is not applied.\n",
      "        disable_env_checker: If to run the env checker, None will default to the environment specification `disable_env_checker`\n",
      "            (which is by default False, running the environment checker),\n",
      "            otherwise will run according to this parameter (`True` = not run, `False` = run)\n",
      "        kwargs: Additional arguments to pass to the environment constructor.\n",
      "    \n",
      "    Returns:\n",
      "        An instance of the environment.\n",
      "    \n",
      "    Raises:\n",
      "        Error: If the ``id`` doesn't exist then an error is raised\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gym.make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m\"\"\"Reset our environment, notice it returns the first frame of the game\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m first_frame \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 10\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(first_frame)\n\u001b[0;32m     12\u001b[0m \u001b[39m\"\"\"Now we can take actions using the env.step function. In breakout the actions are:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m    0 = Stay Still\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    1 = Start Game/Shoot Ball\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m    2 = Move Right\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    3 = Move Left\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"I start the game by step(1), then receive the next frame, reward, done, and info\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\pyplot.py:2623\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2619\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2620\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2621\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2622\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2623\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[0;32m   2624\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[0;32m   2625\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[0;32m   2626\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[0;32m   2627\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2628\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   2629\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m   2630\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2631\u001b[0m     sci(__ret)\n\u001b[0;32m   2632\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5604\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5597\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[0;32m   5598\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[0;32m   5599\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5600\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5601\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5602\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5604\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5605\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5606\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5607\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\matplotlib\\image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> 701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    705\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Create our environment. Basically we define what game we want to play\"\"\"\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "\"\"\"Reset our environment, notice it returns the first frame of the game\"\"\"\n",
    "first_frame = env.reset()\n",
    "plt.imshow(first_frame)\n",
    "\n",
    "\"\"\"Now we can take actions using the env.step function. In breakout the actions are:\n",
    "    0 = Stay Still\n",
    "    1 = Start Game/Shoot Ball\n",
    "    2 = Move Right\n",
    "    3 = Move Left\"\"\"\n",
    "    \n",
    "\"\"\"I start the game by step(1), then receive the next frame, reward, done, and info\"\"\"\n",
    "next_frame, next_frames_reward, next_state_terminal, info = env.step(1)\n",
    "plt.imshow(next_frame)\n",
    "print('Reward Recieved = ' + str(next_frames_reward))\n",
    "print('Next state is a terminal state: ' + str(next_state_terminal))\n",
    "print('info[ale.lives] tells us how many lives we have. Lives: ' + str(info['ale.lives']))\n",
    "\n",
    "\"\"\"Now lets take a bunch of random actions and watch the gameplay using render.\n",
    "If the game ends we will reset it using env.reset\"\"\"\n",
    "\n",
    "for i in range(10000):\n",
    "    a = random.sample([0,1,2,3] , 1)[0]\n",
    "    f_p,r,d,info = env.step(a)\n",
    "    env.render()\n",
    "    if d == True:\n",
    "        env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROZEN LAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "(0, 1)\n",
      "\n",
      "The state sequence: [0, 0, 0, 0, 1, 0, 4, 8, 8, 8, 4, 0, 0, 0, 4, 0, 4, 8, 4, 5]\n",
      "the rewards and total: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 0.0\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [0.0, 0.0, 0.0] 0.0\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [0.0, 0.0, 0.0] 0.0\n",
      "\n",
      "The state sequence: [1, 0, 4, 8, 12]\n",
      "the rewards and total: [0.0, 0.0, 0.0, 0.0, 0.0] 0.0\n",
      "\n",
      "The state sequence: [0, 0, 4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 5]\n",
      "the rewards and total: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39mreward_range)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m     14\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     15\u001b[0m     observations\u001b[39m.\u001b[39mappend(observation)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    371\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    372\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    374\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    376\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\",is_slippery=True)\n",
    "env.action_space.seed(42)\n",
    "count=0\n",
    "observation, info = env.reset(seed=42)\n",
    "rewards=[]\n",
    "observations=[]\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.reward_range)\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    observations.append(observation)\n",
    "    if reward==1:\n",
    "        observation, info = env.reset()\n",
    "        print(\"\\nThe state sequence:\",observations)\n",
    "        print(\"the rewards and total:\",rewards,sum(rewards))\n",
    "        print(count+1)\n",
    "        break\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        print(\"\\nThe state sequence:\",observations)\n",
    "        print(\"the rewards and total:\",rewards,sum(rewards))\n",
    "        count+=1\n",
    "        rewards=[]\n",
    "        observations=[]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROZEN LAKE UPDATED REWARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, info,prob= self.env.step(action)\n",
    "        # change the reward function here\n",
    "        if state in [5,7,11,12]:\n",
    "            reward =-10\n",
    "        elif state==15:\n",
    "            reward=10\n",
    "        else:\n",
    "            reward = -1 # for example\n",
    "        #print(state, reward, done, info,prob)\n",
    "        return state, reward, done, info,prob\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "custom_env = CustomFrozenLakeEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The state sequence: [4, 4, 0, 0, 0, 0, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -10] -18\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 4, 8, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [1, 0, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [4, 8, 4, 8, 8, 4, 8, 9, 13, 9, 10, 14, 13, 13, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -24\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [1, 0, 1, 0, 0, 0, 1, 2, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -19\n",
      "\n",
      "The state sequence: [1, 1, 0, 4, 4, 4, 0, 0, 0, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -21\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [0, 0, 1, 0, 0, 0, 4, 8, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -20\n",
      "\n",
      "The state sequence: [0, 1, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 0, 1, 2, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [1, 2, 2, 6, 10, 9, 8, 4, 8, 9, 8, 9, 10, 11]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -23\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 4, 0, 1, 2, 1, 2, 6, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -10] -18\n",
      "\n",
      "The state sequence: [0, 0, 0, 0, 1, 1, 1, 0, 0, 4, 8, 9, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -22\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [0, 0, 4, 0, 4, 4, 8, 9, 13, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -19\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [4, 4, 0, 1, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [1, 0, 0, 1, 2, 3, 3, 3, 2, 1, 2, 3, 3, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -24\n",
      "\n",
      "The state sequence: [0, 4, 4, 4, 8, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -10] -17\n",
      "\n",
      "The state sequence: [4, 8, 4, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [4, 0, 0, 0, 0, 0, 1, 0, 4, 8, 8, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -22\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -23\n",
      "\n",
      "The state sequence: [0, 0, 4, 0, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [0, 1, 2, 6, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [1, 2, 1, 1, 0, 0, 0, 4, 8, 9, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -21\n",
      "\n",
      "The state sequence: [1, 2, 3, 2, 6, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 1, 2, 6, 10, 6, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [0, 1, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [1, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [4, 4, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [4, 4, 0, 0, 0, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -10] -17\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 0, 4, 8, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [1, 1, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [0, 0, 1, 2, 3, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [0, 0, 0, 4, 0, 4, 8, 4, 8, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -20\n",
      "\n",
      "The state sequence: [0, 0, 0, 4, 0, 0, 1, 2, 3, 2, 2, 1, 0, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -35\n",
      "\n",
      "The state sequence: [1, 0, 0, 0, 0, 0, 1, 2, 6, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -19\n",
      "\n",
      "The state sequence: [1, 0, 4, 4, 8, 9, 8, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -19\n",
      "\n",
      "The state sequence: [4, 8, 9, 10, 14, 14, 13, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -10] -17\n",
      "\n",
      "The state sequence: [0, 0, 1, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 8, 9, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -22\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 1, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -26\n",
      "\n",
      "The state sequence: [0, 0, 4, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [0, 4, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 1, 1, 0, 0, 0, 4, 8, 4, 0, 0, 1, 1, 2, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -25\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [1, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [1, 0, 1, 1, 1, 0, 4, 8, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -19\n",
      "\n",
      "The state sequence: [0, 4, 0, 0, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [1, 2, 2, 2, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [4, 8, 12]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [0, 0, 4, 4, 0, 0, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -10] -18\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [4, 4, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [0, 0, 1, 1, 1, 2, 6, 10, 14, 14, 10, 6, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -22\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 4, 4, 4, 0, 4, 0, 1, 2, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -20\n",
      "\n",
      "The state sequence: [4, 8, 12]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [1, 1, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [0, 0, 1, 2, 2, 6, 2, 3, 2, 3, 3, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -22\n",
      "\n",
      "The state sequence: [0, 0, 0, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [0, 0, 4, 0, 0, 0, 4, 0, 1, 2, 2, 6, 10, 9, 13, 9, 8, 4, 4, 0, 0, 4, 4, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -35\n",
      "\n",
      "The state sequence: [1, 1, 0, 4, 0, 4, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -10] -17\n",
      "\n",
      "The state sequence: [0, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -10] -13\n",
      "\n",
      "The state sequence: [0, 1, 0, 0, 0, 4, 8, 8, 9, 10, 11]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -20\n",
      "\n",
      "The state sequence: [0, 4, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n",
      "\n",
      "The state sequence: [4, 0, 0, 4, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [0, 4, 0, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -10] -14\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 1, 2, 6, 10, 6, 2, 2, 1, 2, 3, 3, 3, 3, 2, 3, 7]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10] -26\n",
      "\n",
      "The state sequence: [1, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [0, 0, 0, 4, 8, 9, 10, 11]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -10] -17\n",
      "\n",
      "The state sequence: [0, 0, 4, 4, 0, 4, 4, 8, 12]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -1, -1, -10] -18\n",
      "\n",
      "The state sequence: [0, 4, 0, 0, 0, 1, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -1, -10] -16\n",
      "\n",
      "The state sequence: [4, 0, 4, 8, 9, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [1, 0, 4, 8, 4, 5]\n",
      "the rewards and total: [-1, -1, -1, -1, -1, -10] -15\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [4, 5]\n",
      "the rewards and total: [-1, -10] -11\n",
      "\n",
      "The state sequence: [4, 4, 5]\n",
      "the rewards and total: [-1, -1, -10] -12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m custom_env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m     13\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     14\u001b[0m     observations\u001b[39m.\u001b[39mappend(observation)\n",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m, in \u001b[0;36mCustomFrozenLakeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m----> 6\u001b[0m     state, reward, done, info,prob\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m      7\u001b[0m     \u001b[39m# change the reward function here\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m state \u001b[39min\u001b[39;00m [\u001b[39m5\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m11\u001b[39m,\u001b[39m12\u001b[39m]:\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32md:\\tools2\\anaconda\\envs\\gym\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    371\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    372\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    374\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    376\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "done = False\n",
    "state = custom_env.reset()\n",
    "rewards=[]\n",
    "#while not done:\n",
    "#    action = np.random.choice([0, 1, 2, 3]) # for example\n",
    "#    state, reward, done, info,prob = custom_env.step(env.action_space.sample())\n",
    " #   custom_env.render()\n",
    "observations=[]\n",
    "count=0\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = custom_env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    observations.append(observation)\n",
    "    if reward==10:\n",
    "        observation, info = custom_env.reset()\n",
    "        print(\"\\nThe state sequence:\",observations)\n",
    "        print(\"the rewards and total:\",rewards,sum(rewards))\n",
    "        print(count+1)\n",
    "        break\n",
    "    if terminated or truncated:\n",
    "        observation, info = custom_env.reset()\n",
    "        print(\"\\nThe state sequence:\",observations)\n",
    "        print(\"the rewards and total:\",rewards,sum(rewards))\n",
    "        count+=1\n",
    "        rewards=[]\n",
    "        observations=[]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00449108 0.00515001 0.01243038 0.00378316 0.00960639 0.\n",
      " 0.03569265 0.         0.02707277 0.07934699 0.14053814 0.\n",
      " 0.         0.17244764 0.48726667 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "discount_factor = 0.7\n",
    "P = np.zeros((env.observation_space.n, env.action_space.n, env.observation_space.n))\n",
    "\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        for transition in env.P[s][a]:\n",
    "            prob, next_state, reward, done = transition\n",
    "            P[s, a, next_state] = prob\n",
    "\n",
    "R = np.zeros((env.observation_space.n, env.action_space.n, env.observation_space.n))\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        for transition in env.P[s][a]:\n",
    "            prob, next_state, reward, done = transition\n",
    "            R[s, a, next_state] = reward\n",
    "\n",
    "def value_iteration(P, R, discount_factor):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for i in range(100):\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for s in range(env.observation_space.n):\n",
    "            for a in range(env.action_space.n):\n",
    "                for next_s in range(env.observation_space.n):\n",
    "                    Q[s, a] += P[s, a, next_s] * (R[s, a, next_s] + discount_factor * V[next_s])\n",
    "                    \n",
    "        V = np.max(Q, axis=1)\n",
    "    return V\n",
    "\n",
    "# Compute the optimal value function\n",
    "\n",
    "V = value_iteration(P, R, discount_factor)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n",
      " 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n",
      " 0.         0.3799359  0.63902015 0.        ]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "\n",
    "value_function = np.zeros(env.observation_space.n)\n",
    "policy = np.zeros(env.observation_space.n)\n",
    "discount_factor = 0.9\n",
    "\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    new_value_function = np.copy(value_function)\n",
    "    for state in range(env.observation_space.n):\n",
    "        action_values =np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for transition_prob, next_state, reward, done in env.P[state][action]:\n",
    "                action_values[action] += transition_prob * (reward + discount_factor * new_value_function[next_state])\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state] = int(best_action)\n",
    "        policy=np.asarray(policy,dtype=\"int\")\n",
    "        new_value_function[state] = action_values[best_action]\n",
    "    value_function = np.copy(new_value_function)\n",
    "\n",
    "env.reset()\n",
    "state=0\n",
    "print(value_function)\n",
    "done = False\n",
    "while not done:\n",
    "    action = policy[state]\n",
    "    print(action)\n",
    "    next_state, reward, done, _,a = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5e3731ecaf263f433863a300b375545153b02e8730af39d7770c49b4646125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
